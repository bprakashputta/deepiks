{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645d9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import sklearn.metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "991c4ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_input (InputLayer)    [(None, 300, 300, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 150, 150, 32)      2432      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 75, 75, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 75, 75, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 37, 37, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               21234176  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 23,067,970\n",
      "Trainable params: 23,067,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=load_model(\"sam.h5\")\n",
    "model = tf.keras.models.Model(inputs=[model.input], outputs=[model.output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6380ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIMS = (300, 300, 3)\n",
    "batch_size=64\n",
    "test_dir = r'F:/Work/casting_data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "240ca8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 715 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "##### Renaming of batch size concept for testing purpose #\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.)\n",
    "\n",
    "n_test = sum([len(files) for r, d, files in os.walk(test_dir)])\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(directory = test_dir,\n",
    "                                                 # batch_size = n_test,\n",
    "                                                batch_size = 1,\n",
    "                                                  target_size = (IMAGE_DIMS[0], IMAGE_DIMS[1]),\n",
    "                                                  class_mode = \"categorical\",\n",
    "                                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c886102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = test_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28469cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Temp/ipykernel_16780/1280859773.py:1: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict_generator(test_generator[101][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d0b9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(test_generator[101][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba439cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93267065, 0.0673293 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e6c3d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a825eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image \n",
    "# convert into array \n",
    "# reshape and input into predict function\n",
    "\n",
    "\n",
    "# open method used to open different extension image file\n",
    "#im = Image.open(r\"C:/Users/dell/Desktop/NDS/PI/def_front_bearing.png\") \n",
    "im = Image.open(r\"C:/Users/dell/Desktop/NDS/PI/ok_front_bearing.png\")  \n",
    "# This method will show image in any image viewer \n",
    "im.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5229b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "imgArray = np.array(im)\n",
    "print(imgArray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd8273f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "print(imgArray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5cf576bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgArray = np.expand_dims(imgArray, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "679c2765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300, 300, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgArray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f54fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred= model.predict(imgArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9303f122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a853f7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[177, 177, 177],\n",
       "         [177, 177, 177],\n",
       "         [177, 177, 177],\n",
       "         ...,\n",
       "         [177, 177, 177],\n",
       "         [176, 176, 176],\n",
       "         [176, 176, 176]],\n",
       "\n",
       "        [[178, 178, 178],\n",
       "         [177, 177, 177],\n",
       "         [177, 177, 177],\n",
       "         ...,\n",
       "         [172, 172, 172],\n",
       "         [172, 172, 172],\n",
       "         [173, 173, 173]],\n",
       "\n",
       "        [[178, 178, 178],\n",
       "         [178, 178, 178],\n",
       "         [177, 177, 177],\n",
       "         ...,\n",
       "         [170, 170, 170],\n",
       "         [171, 171, 171],\n",
       "         [172, 172, 172]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[155, 155, 155],\n",
       "         [154, 154, 154],\n",
       "         [153, 153, 153],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [144, 144, 144],\n",
       "         [143, 143, 143]],\n",
       "\n",
       "        [[151, 151, 151],\n",
       "         [152, 152, 152],\n",
       "         [153, 153, 153],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [144, 144, 144],\n",
       "         [143, 143, 143]],\n",
       "\n",
       "        [[150, 150, 150],\n",
       "         [152, 152, 152],\n",
       "         [154, 154, 154],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [144, 144, 144],\n",
       "         [143, 143, 143]]]], dtype=uint8)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "caa215c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.6392157 , 0.6392157 , 0.6392157 ],\n",
       "         [0.64705884, 0.64705884, 0.64705884],\n",
       "         [0.6627451 , 0.6627451 , 0.6627451 ],\n",
       "         ...,\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ],\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ],\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ]],\n",
       "\n",
       "        [[0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.64705884, 0.64705884, 0.64705884],\n",
       "         [0.65882355, 0.65882355, 0.65882355],\n",
       "         ...,\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ],\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ],\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ]],\n",
       "\n",
       "        [[0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.6431373 , 0.6431373 , 0.6431373 ],\n",
       "         [0.654902  , 0.654902  , 0.654902  ],\n",
       "         ...,\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ],\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ],\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5294118 , 0.5294118 , 0.5294118 ],\n",
       "         [0.53333336, 0.53333336, 0.53333336],\n",
       "         [0.53333336, 0.53333336, 0.53333336],\n",
       "         ...,\n",
       "         [0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.63529414, 0.63529414, 0.63529414]],\n",
       "\n",
       "        [[0.5372549 , 0.5372549 , 0.5372549 ],\n",
       "         [0.5372549 , 0.5372549 , 0.5372549 ],\n",
       "         [0.5372549 , 0.5372549 , 0.5372549 ],\n",
       "         ...,\n",
       "         [0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.63529414, 0.63529414, 0.63529414]],\n",
       "\n",
       "        [[0.54509807, 0.54509807, 0.54509807],\n",
       "         [0.54509807, 0.54509807, 0.54509807],\n",
       "         [0.54509807, 0.54509807, 0.54509807],\n",
       "         ...,\n",
       "         [0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.63529414, 0.63529414, 0.63529414]]]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator[101][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bf991b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgArray = imgArray.reshape((-1, 300, 300, \n",
    "                            3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "adc42dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300, 300, 3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgArray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d76ed87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred= model.predict(imgArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "029622da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ac989bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array(test_generator[101][0][0])\n",
    "data1 = Image.fromarray((arr1 * 255).astype(np.uint8))\n",
    "data1.save('def_front_bearing1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7f591942",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgArray=imgArray/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5743490e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.69411765, 0.69411765, 0.69411765],\n",
       "         [0.69411765, 0.69411765, 0.69411765],\n",
       "         [0.69411765, 0.69411765, 0.69411765],\n",
       "         ...,\n",
       "         [0.69411765, 0.69411765, 0.69411765],\n",
       "         [0.69019608, 0.69019608, 0.69019608],\n",
       "         [0.69019608, 0.69019608, 0.69019608]],\n",
       "\n",
       "        [[0.69803922, 0.69803922, 0.69803922],\n",
       "         [0.69411765, 0.69411765, 0.69411765],\n",
       "         [0.69411765, 0.69411765, 0.69411765],\n",
       "         ...,\n",
       "         [0.6745098 , 0.6745098 , 0.6745098 ],\n",
       "         [0.6745098 , 0.6745098 , 0.6745098 ],\n",
       "         [0.67843137, 0.67843137, 0.67843137]],\n",
       "\n",
       "        [[0.69803922, 0.69803922, 0.69803922],\n",
       "         [0.69803922, 0.69803922, 0.69803922],\n",
       "         [0.69411765, 0.69411765, 0.69411765],\n",
       "         ...,\n",
       "         [0.66666667, 0.66666667, 0.66666667],\n",
       "         [0.67058824, 0.67058824, 0.67058824],\n",
       "         [0.6745098 , 0.6745098 , 0.6745098 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.60784314, 0.60784314, 0.60784314],\n",
       "         [0.60392157, 0.60392157, 0.60392157],\n",
       "         [0.6       , 0.6       , 0.6       ],\n",
       "         ...,\n",
       "         [0.56862745, 0.56862745, 0.56862745],\n",
       "         [0.56470588, 0.56470588, 0.56470588],\n",
       "         [0.56078431, 0.56078431, 0.56078431]],\n",
       "\n",
       "        [[0.59215686, 0.59215686, 0.59215686],\n",
       "         [0.59607843, 0.59607843, 0.59607843],\n",
       "         [0.6       , 0.6       , 0.6       ],\n",
       "         ...,\n",
       "         [0.56862745, 0.56862745, 0.56862745],\n",
       "         [0.56470588, 0.56470588, 0.56470588],\n",
       "         [0.56078431, 0.56078431, 0.56078431]],\n",
       "\n",
       "        [[0.58823529, 0.58823529, 0.58823529],\n",
       "         [0.59607843, 0.59607843, 0.59607843],\n",
       "         [0.60392157, 0.60392157, 0.60392157],\n",
       "         ...,\n",
       "         [0.56862745, 0.56862745, 0.56862745],\n",
       "         [0.56470588, 0.56470588, 0.56470588],\n",
       "         [0.56078431, 0.56078431, 0.56078431]]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5dfe1363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_dir = r'F:/sampletest'\n",
    "test_generator = test_datagen.flow_from_directory(directory =test_dir,\n",
    "                                                 # batch_size = n_test,\n",
    "                                                batch_size = 1,\n",
    "                                                  target_size = (300, 300),\n",
    "                                                  #class_mode = \"categorical\",\n",
    "                                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "145b3ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.69411767, 0.69411767, 0.69411767],\n",
       "         [0.69411767, 0.69411767, 0.69411767],\n",
       "         [0.69411767, 0.69411767, 0.69411767],\n",
       "         ...,\n",
       "         [0.69411767, 0.69411767, 0.69411767],\n",
       "         [0.6901961 , 0.6901961 , 0.6901961 ],\n",
       "         [0.6901961 , 0.6901961 , 0.6901961 ]],\n",
       "\n",
       "        [[0.69803923, 0.69803923, 0.69803923],\n",
       "         [0.69411767, 0.69411767, 0.69411767],\n",
       "         [0.69411767, 0.69411767, 0.69411767],\n",
       "         ...,\n",
       "         [0.6745098 , 0.6745098 , 0.6745098 ],\n",
       "         [0.6745098 , 0.6745098 , 0.6745098 ],\n",
       "         [0.6784314 , 0.6784314 , 0.6784314 ]],\n",
       "\n",
       "        [[0.69803923, 0.69803923, 0.69803923],\n",
       "         [0.69803923, 0.69803923, 0.69803923],\n",
       "         [0.69411767, 0.69411767, 0.69411767],\n",
       "         ...,\n",
       "         [0.6666667 , 0.6666667 , 0.6666667 ],\n",
       "         [0.67058825, 0.67058825, 0.67058825],\n",
       "         [0.6745098 , 0.6745098 , 0.6745098 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.60784316, 0.60784316, 0.60784316],\n",
       "         [0.6039216 , 0.6039216 , 0.6039216 ],\n",
       "         [0.6       , 0.6       , 0.6       ],\n",
       "         ...,\n",
       "         [0.5686275 , 0.5686275 , 0.5686275 ],\n",
       "         [0.5647059 , 0.5647059 , 0.5647059 ],\n",
       "         [0.56078434, 0.56078434, 0.56078434]],\n",
       "\n",
       "        [[0.5921569 , 0.5921569 , 0.5921569 ],\n",
       "         [0.59607846, 0.59607846, 0.59607846],\n",
       "         [0.6       , 0.6       , 0.6       ],\n",
       "         ...,\n",
       "         [0.5686275 , 0.5686275 , 0.5686275 ],\n",
       "         [0.5647059 , 0.5647059 , 0.5647059 ],\n",
       "         [0.56078434, 0.56078434, 0.56078434]],\n",
       "\n",
       "        [[0.5882353 , 0.5882353 , 0.5882353 ],\n",
       "         [0.59607846, 0.59607846, 0.59607846],\n",
       "         [0.6039216 , 0.6039216 , 0.6039216 ],\n",
       "         ...,\n",
       "         [0.5686275 , 0.5686275 , 0.5686275 ],\n",
       "         [0.5647059 , 0.5647059 , 0.5647059 ],\n",
       "         [0.56078434, 0.56078434, 0.56078434]]]], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "374f63b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21953058, 0.7804694 ]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred= model.predict(test_generator[0][0])\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1c6cb45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "89919e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300, 300, 3)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "88b4d3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21953058, 0.7804694 ]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred= model.predict(test_generator[1][0])\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cceaf5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300, 3)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "59192bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300, 300, 3)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have to store an image into directory and have to pass the array value that resprensts the array object#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1c5948a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.)\n",
    "test_dir = r'F:/sampletest'\n",
    "test_generator = test_datagen.flow_from_directory(directory =test_dir,\n",
    "                                                 # batch_size = n_test,\n",
    "                                                batch_size = 1,\n",
    "                                                  target_size = (300, 300),\n",
    "                                                  #class_mode = \"categorical\",\n",
    "                                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "41c2d41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93267065, 0.0673293 ]], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred= model.predict(test_generator[0][0])\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "573e6618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.6392157 , 0.6392157 , 0.6392157 ],\n",
       "         [0.64705884, 0.64705884, 0.64705884],\n",
       "         [0.6627451 , 0.6627451 , 0.6627451 ],\n",
       "         ...,\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ],\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ],\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ]],\n",
       "\n",
       "        [[0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.64705884, 0.64705884, 0.64705884],\n",
       "         [0.65882355, 0.65882355, 0.65882355],\n",
       "         ...,\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ],\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ],\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ]],\n",
       "\n",
       "        [[0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.6431373 , 0.6431373 , 0.6431373 ],\n",
       "         [0.654902  , 0.654902  , 0.654902  ],\n",
       "         ...,\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ],\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ],\n",
       "         [0.7372549 , 0.7372549 , 0.7372549 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5294118 , 0.5294118 , 0.5294118 ],\n",
       "         [0.53333336, 0.53333336, 0.53333336],\n",
       "         [0.53333336, 0.53333336, 0.53333336],\n",
       "         ...,\n",
       "         [0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.63529414, 0.63529414, 0.63529414]],\n",
       "\n",
       "        [[0.5372549 , 0.5372549 , 0.5372549 ],\n",
       "         [0.5372549 , 0.5372549 , 0.5372549 ],\n",
       "         [0.5372549 , 0.5372549 , 0.5372549 ],\n",
       "         ...,\n",
       "         [0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.63529414, 0.63529414, 0.63529414]],\n",
       "\n",
       "        [[0.54509807, 0.54509807, 0.54509807],\n",
       "         [0.54509807, 0.54509807, 0.54509807],\n",
       "         [0.54509807, 0.54509807, 0.54509807],\n",
       "         ...,\n",
       "         [0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.63529414, 0.63529414, 0.63529414],\n",
       "         [0.63529414, 0.63529414, 0.63529414]]]], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7665d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
